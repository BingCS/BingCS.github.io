<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Projects | WANG Bing </title> <meta name="author" content="WANG Bing"> <meta name="description" content="Ongoing and completed research projects by SPACE Group"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/WB.png?122d6e99985673b20e1df61cab899150"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bingcs.github.io/research/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">WANG</span> Bing </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">SPACE </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/research/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/vacancies/">Vacancies </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/links/">Links</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/contact/">Contact</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Projects</h1> <p class="post-description">Ongoing and completed research projects by SPACE Group</p> </header> <article> <hr> <section> <h4>Hierarchical Intermediate Feature Description and Alignment across Point Clouds and Images without Human Annotations</h4> Precisely interpreting the inherent complementarity between point clouds and images is essential for machines to effectively perceive the surrounding environment. Central to this is the careful establishment of robust and consistent associations between the two modalities. Previous methods have treated this task as fully-supervised metric learning or pose optimization problems, thus heavily relying on dense human annotations for network training. Despite impressive performance, these methods are prohibitively costly and struggle to generalize to unseen real-world scenarios. Additionally, they typically employ straightforward alignment strategies, intrinsically overlooking the feature granularity necessary for hierarchical alignment, especially in the presence of significant variations or intricate structures. This project aims to align point clouds and images without the need for human labels during training. This is particularly challenging and remains largely unexplored, even with the rapid advancements of full supervision methods. The primary challenge arises from the lack of intermediate features. To address this, we will derive semantic and geometric insights from raw point clouds and images, and subsequently integrate meaningful intra- and cross-modal consensuses as supervision signals for training neural networks. Our research will pioneer several innovative techniques. Firstly, we will design a new taskagnostic pipeline to construct unified features for aligning point clouds and images at a group level. This pipeline will serve as a bridge between the two modalities, leveraging both taskagnostic, group-coherent semantic and geometric cues to effectively identify unified features. Secondly, a novel self-supervised learning framework will be introduced to learn transferable features for patch-level alignment without human annotations. It will leverage both intra- and cross-modal transferability to infer the invariance between point cloud and image patches. Thirdly, another new self-supervised architecture will be proposed to identify point-pixel correspondences using consensus-driven features. It will harness global and cross-modal consensuses to interact these features, thereby enabling adaptive point-pixel alignment. This project is poised to directly advance multi-modal and self-supervised learning techniques for the interpretation and association of point clouds and images. Our methods aim to learn universal intermediate features rather than merely fitting to human labels, thus holding the potential to transform how machines perceive and integrate information across modalities. This paves the way for ground-breaking advancements in domains that require comprehensive scene understanding. Furthermore, this project is anticipated to influence cutting-edge applications such as autonomous driving, robotic manipulation, and augmented reality. </section> <hr> <section> <h4>Multi-modal Fusion for Autonomous and Intelligent Underwater Localization and Mapping</h4> In the rapidly evolving coastal metropolises, particularly Hong Kong, Marine Surveying is critical for navigating and managing the extensive coastline and bustling port activities. The unique marine environment of Hong Kong, characterized by its strategic maritime position and heavy shipping traffic, necessitates advanced methodologies to ensure navigational safety and environmental monitoring. This research initiative aims to transform Marine Surveying through the development and deployment of innovative and intelligent multimodal robotic systems, specifically Underwater Unmanned Vehicles (UUVs). These systems are tailored for autonomous operation in the complex marine environment of Hong Kong, incorporating state-of-the-art advancements in Spatial Perception for multimodal localization and mapping. This enables the UUVs to survey and map the marine terrain accurately, providing essential data for maritime navigation and environmental assessments. The impact of this research is profound, as it introduces a pioneering approach to Marine Surveying using UUVs equipped with cutting-edge technologies. By improving survey accuracy and efficiency, these systems play an essential role in promoting sustainability, mitigating safety risks, and enhancing navigational safety. This project represents a commitment to innovative and sustainable solutions, positioning the region as a leader in technological advancement in the field of underwater robotics. Overall, multi-modal spatial perception in UUVs for Marine Surveying has the potential to advance the field of underwater robotics, contribute to research in control theory and computer vision, and result in safer, more cost-effective maritime operations. The broader impact of this research, if successfully implemented, could extend beyond Hong Kong, establishing a global standard for Marine Surveying. </section> <hr> <section> <h4>Local Feature Extraction and Matching for Cross-modal Registration</h4> Accurate cross-modal registration between real-scene images and 3D point clouds is the key to ensuring the quality of spatial location services and smart city applications. The core problem in cross-modal registration is local feature extraction and matching. Existing methods mainly train dedicated neural networks for specific single-modal registration tasks, which are unable to describe the shared features between images and point clouds, detect keypoints based on a unified standard, and lack the ability to match features with global consistency. This project aims to propose a new integrated framework of cross-modal local shared feature description, keypoint detection and feature matching for real-scene image and 3D point cloud registration. Firstly, an adaptive feature description framework is constructed based on pseudo-siamese neural networks to obtain a discriminative and compact local feature description algorithm. Secondly, a self-supervised keypoint detection mechanism and corresponding implementation strategy are designed based on non-maximal suppression to obtain a unique and repeatable keypoint detection algorithm. Finally, an optimal transport feature matching model is established based on attentional graph neural networks to obtain a globally optimal and efficient feature matching algorithm. This research will effectively promote the cross fusion of cross-modal learning, metric learning and deep learning, and critically provide theoretical support and technology guarantee for the practical application of cross-modal registration methods, which has essential academic significance and broad application value. </section> <hr> <section> <h4>Unsupervised Metric-Semantic Understanding from Large-Scale 3D Point Clouds</h4> Recent years have seen a growing interest towards metric-semantic understanding, which consists in building a semantically annotated (or object-oriented) model of the environment. Precisely understanding the shape and semantic compositions of large-scale 3D point clouds is crucial for machines to quickly interpret the surrounding environment. Core tasks include semantic segmentation, object detection/segmentation, surface reconstruction, etc. Previous deep learning methods consider these tasks as fully-supervised classification or regression problems, thus relying heavily on dense human annotations for training networks. Although achieving great performance, they are prohibitively costly and can hardly generalize to unseen scenarios for real-world deployment. For example, it takes 1700 person-hours to annotate a typical outdoor point cloud dataset (SemanticKITTI) and 600 person-hours an indoor dataset (ScanNet), and the models trained on an indoor dataset are barely able to generalize to outdoor scenes. This project aims to interpret 3D scenes without requiring any human labels in training. This is particularly challenging and highly unexplored thus far, despite the rapid progress of fullsupervision methods. Fundamentally, the challenge lies in the lack of explicit supervision signals. To address this challenge, we will introduce semantic and shape priors from raw point clouds and then integrate meaningful inductive biases as supervision signals to train neural networks. This research will develop several technical innovations. First, a new unsupervised pipeline will be designed to learn both 3D semantics and object instances without needing human annotations. This pipeline will exploit shape similarities to infer per-point semantic categories, and leverage motion cues to discover object boundaries. Second, a novel zero-shot learning framework will be introduced to push the boundary of segmenting open-world point clouds in which some object classes are never seen in training. This framework will utilize the semantic connection between textual and visual embeddings and then establish the proximity between seen and unseen classes. Third, another new unsupervised architecture will be proposed to recover continuous 3D surfaces only from discrete point cloud scans. This architecture will exploit the shape continuity of local regions as pseudo labels to train a surface reconstruction network. This project will directly advance the unsupervised and zero-shot learning techniques for analyzing large-scale point clouds. In addition, our methods are planned to learn general 3D point features instead of fitting human labels, and therefore have the potential to improve existing fullysupervised methods when integrated properly. It is also expected to have a practical impact on cutting-edge applications such as autonomous driving, navigation, and mixed reality. </section> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 WANG Bing. Last Updated: September 01, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script>var wechatModal=document.getElementById("WeChatMod"),wechatBtn=document.getElementById("WeChatBtn");wechatBtn.onclick=function(){wechatModal.style.display="block"},window.onclick=function(t){t.target==wechatModal&&(wechatModal.style.display="none")};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>